{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Sobel + YOLO + FaceMesh + Xception) - Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import pretrainedmodels  # For Xception model\n",
    "from ultralytics import YOLO  # For YOLOv8 face detection\n",
    "import ssl\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline below is for WildRF -- Can be extended Similarly for CollabDiff and eVe StyleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1732399347.913995  238834 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732399347.924421  238834 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.WARNING)\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations for Xception\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((299, 299)),  # Xception requires 299x299 input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Set up Mediapipe for facial landmarks extraction\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1)\n",
    "\n",
    "# Loading YOLOv8 model\n",
    "yolo_model = YOLO(\"yolov8n.pt\").to(device)  # Ensure YOLO runs on GPU if available)  # Choose the YOLOv8 model variant based on resources\n",
    "# Loading Xception model\n",
    "xception_model = pretrainedmodels.__dict__['xception'](pretrained='imagenet').to(device)\n",
    "xception_model.last_linear = torch.nn.Linear(xception_model.last_linear.in_features, 128).to(device)  # Adjust for feature extraction\n",
    "\n",
    "# Define COCO classes we are interested in (people, vehicles, animals, household items, etc.)\n",
    "COCO_CLASSES = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\",\n",
    "    \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\",\n",
    "    \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
    "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\",\n",
    "    \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
    "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "    \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
    "    \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "# Dataset Class\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None, limit=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self._load_data(limit)\n",
    "\n",
    "    def _load_data(self, limit):\n",
    "        if self.split == 'test':\n",
    "            for platform in ['facebook', 'reddit', 'twitter']:\n",
    "                for label in ['0_real', '1_fake']:\n",
    "                    path = os.path.join(self.root_dir, self.split, platform, label)\n",
    "                    self._load_images(path, label, limit)\n",
    "        else:\n",
    "            for label in ['0_real', '1_fake']:\n",
    "                path = os.path.join(self.root_dir, self.split, label)\n",
    "                self._load_images(path, label, limit)\n",
    "\n",
    "    def _load_images(self, path, label, limit):\n",
    "        if os.path.exists(path):\n",
    "            for i, img_name in enumerate(os.listdir(path)):\n",
    "                if limit and len(self.data) >= limit:\n",
    "                    break\n",
    "                self.data.append(os.path.join(path, img_name))\n",
    "                self.labels.append(0 if label == '0_real' else 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # YOLOv8 for object detection\n",
    "        results = yolo_model(image)\n",
    "        detected_objects = []\n",
    "        face_landmarks = np.zeros((936,), dtype=np.float32) \n",
    "\n",
    "        for result in results[0].boxes:\n",
    "            class_id = int(result.cls[0])  # YOLOv8 returns class IDs\n",
    "            # class_name = YOLO.names[class_id]  # Get class name from YOLO COCO classes\n",
    "            class_name = yolo_model.names[class_id] \n",
    "\n",
    "            # Check if the detected object is one of the COCO classes we care about\n",
    "            if class_name in COCO_CLASSES:\n",
    "                x1, y1, x2, y2 = result.xyxy[0].cpu().numpy()\n",
    "                obj_crop = image[int(y1):int(y2), int(x1):int(x2)]\n",
    "                detected_objects.append(class_id)\n",
    "\n",
    "                # If the detected object is a person, get facial landmarks\n",
    "                if class_name == 'person':\n",
    "                    results_face = face_mesh.process(cv2.cvtColor(obj_crop, cv2.COLOR_BGR2RGB))\n",
    "                    if results_face.multi_face_landmarks:\n",
    "                        face_landmarks = np.array([[p.x, p.y] for p in results_face.multi_face_landmarks[0].landmark])\n",
    "                        face_landmarks = face_landmarks.flatten()\n",
    "\n",
    "        # Encode detected objects as a one-hot vector of COCO class detections\n",
    "        yolo_features = np.zeros(len(COCO_CLASSES))\n",
    "        for obj_id in detected_objects:\n",
    "            yolo_features[obj_id] = 1  # Mark the detected class in the one-hot vector\n",
    "        \n",
    "        # Applying Sobel edge detection\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        sobel_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        sobel_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "        sobel_combined = cv2.magnitude(sobel_x, sobel_y)\n",
    "        sobel_combined = cv2.convertScaleAbs(sobel_combined)\n",
    "        sobel_combined = cv2.merge([sobel_combined, sobel_combined, sobel_combined])\n",
    "        # Transform the image for Xception model\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            sobel_combined = self.transform(sobel_combined)\n",
    "\n",
    "        yolo_features = torch.tensor(yolo_features, dtype=torch.float32).to(device)\n",
    "        face_landmarks = torch.tensor(face_landmarks, dtype=torch.float32).to(device)\n",
    "\n",
    "        return image.to(device), sobel_combined.to(device), yolo_features, face_landmarks, torch.tensor(label, dtype=torch.long).to(device)\n",
    "\n",
    "# Define the Classifier Model using Xception\n",
    "class DeepfakeClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepfakeClassifier, self).__init__()\n",
    "        self.xception = xception_model  # Outputs 128 features\n",
    "        self.sobel_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.sobel_linear = None  # Will initialize dynamically\n",
    "        self.fc_landmarks = nn.Linear(936, 128).to(device)  # 936 = flattened landmarks\n",
    "        self.fc_yolo = nn.Linear(80, 64).to(device)  # Adjust YOLO features to 64\n",
    "        self.fc1 = None  # To be initialized dynamically\n",
    "        self.fc2 = nn.Linear(128, 2).to(device)\n",
    "\n",
    "    def initialize_sobel_linear(self, input_shape):\n",
    "        with torch.no_grad():\n",
    "            # Initialize Sobel Linear\n",
    "            sample_input = torch.zeros(1, *input_shape).to(device)\n",
    "            output = self.sobel_cnn(sample_input)\n",
    "            flattened_size = output.view(-1).size(0)\n",
    "            self.sobel_linear = nn.Linear(flattened_size, 128).to(device)\n",
    "\n",
    "            # Calculate the total feature size for fc1\n",
    "            total_feature_size = 128 + 128 + 128 + 64  # xception + sobel + landmarks + YOLO\n",
    "            self.fc1 = nn.Linear(total_feature_size, 128).to(device)\n",
    "\n",
    "    def forward(self, image, sobel_image, yolo_features, face_landmarks):\n",
    "        # Process features\n",
    "        \n",
    "        image_features = self.xception(image)  # Output: [batch_size, 128]\n",
    "        sobel_features = self.sobel_cnn(sobel_image)  # Output: [batch_size, C, H, W]\n",
    "        sobel_features = self.sobel_linear(sobel_features.view(sobel_features.size(0), -1))\n",
    "        yolo_features = torch.relu(self.fc_yolo(yolo_features))\n",
    "        landmark_features = torch.relu(self.fc_landmarks(face_landmarks))\n",
    "\n",
    "        # Combine features\n",
    "        combined = torch.cat((image_features, sobel_features, yolo_features, landmark_features), dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "\n",
    "# Dataset and DataLoader\n",
    "root_dir = 'WildRF'  # Replace with actual path\n",
    "train_dataset = DeepfakeDataset(root_dir=root_dir, split='train', transform=transform)\n",
    "val_dataset = DeepfakeDataset(root_dir=root_dir, split='val', transform=transform)\n",
    "test_dataset = DeepfakeDataset(root_dir=root_dir, split='test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "model = DeepfakeClassifier().to(device)\n",
    "# Dynamically calculate the flattened size for sobel_cnn\n",
    "model.initialize_sobel_linear(input_shape=(3, 299, 299))  # Assuming Sobel image size is 299x299\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Hyperparameter (tunable)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 3  # Stop if no improvement in validation loss after 5 epochs\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Lists to store accuracy and loss values for plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_model_path = \"best_model_module1.pth\"\n",
    "\n",
    "# Training Loop with validation and early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, sobel_images, yolo_features, landmarks, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, sobel_images, yolo_features, landmarks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_train_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "    \n",
    "    epoch_train_loss = running_train_loss / len(train_loader)\n",
    "    epoch_train_accuracy = correct_train / total_train\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_accuracy)\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, sobel_images, yolo_features, landmarks, labels in tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Validation\"):\n",
    "            outputs = model(images, sobel_images, yolo_features, landmarks)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader)\n",
    "    epoch_val_accuracy = correct_val / total_val\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_accuracies.append(epoch_val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_accuracy * 100:.2f}%, \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {epoch_val_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Early Stopping and Model Selection\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), best_model_path)  # Save the best model\n",
    "        print(f\"Best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model for final testing or further evaluation\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "print(\"Loaded the best model based on validation performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label=\"Training Accuracy\", marker='o')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label=\"Validation Accuracy\", marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"training_validation_accuracy_module1_WildRf.png\")\n",
    "print(\"Training and Validation accuracy plot saved as 'training_validation_accuracy_module1_WildRf.png'.\")\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", marker='o')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"training_validation_loss_module1_WildRf.png\")\n",
    "print(\"Training and Validation loss plot saved as 'training_validation_loss_module1_WildRf.png'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing onn facebook, twitter, reddit and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the best model for testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing on Test Set:  52%|█████▏    | 81/157 [18:51<14:44, 11.64s/it]  libpng warning: iCCP: known incorrect sRGB profile\n",
      "Testing on Test Set:  55%|█████▍    | 86/157 [19:30<08:56,  7.55s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
      "Testing on Test Set:  56%|█████▌    | 88/157 [19:43<08:05,  7.03s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
      "Testing on Test Set:  62%|██████▏   | 98/157 [21:09<11:12, 11.41s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
      "Testing on Test Set:  67%|██████▋   | 105/157 [21:43<04:58,  5.75s/it]libpng warning: iCCP: known incorrect sRGB profile\n",
      "Testing on Test Set: 100%|██████████| 157/157 [28:16<00:00, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2924, Test Accuracy: 87.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"Loaded the best model for testing.\")\n",
    "\n",
    "# Test Phase\n",
    "running_test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, sobel_images, yolo_features, landmarks, labels in tqdm(test_loader, desc=\"Testing on Test Set\"):\n",
    "        outputs = model(images, sobel_images, yolo_features, landmarks)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_test_loss += loss.item()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_test += (preds == labels).sum().item()\n",
    "        total_test += labels.size(0)\n",
    "\n",
    "test_loss = running_test_loss / len(test_loader)\n",
    "test_accuracy = correct_test / total_test\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake Image 1 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_1.png\n",
      "Fake Image 2 - Predicted Class: 0, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_2.png\n",
      "Fake Image 3 - Predicted Class: 0, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_3.png\n",
      "Fake Image 4 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_4.png\n",
      "Fake Image 5 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_5.png\n",
      "Fake Image 6 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_6.png\n",
      "Fake Image 7 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_7.png\n",
      "Fake Image 8 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_8.png\n",
      "Fake Image 9 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_9.png\n",
      "Fake Image 10 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_10.png\n",
      "Fake Image 11 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_11.png\n",
      "Fake Image 12 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_12.png\n",
      "Fake Image 13 - Predicted Class: 0, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_13.png\n",
      "Fake Image 14 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_14.png\n",
      "Fake Image 15 - Predicted Class: 0, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_15.png\n",
      "Fake Image 16 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_16.png\n",
      "Fake Image 17 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_17.png\n",
      "Fake Image 18 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_18.png\n",
      "Fake Image 19 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_19.png\n",
      "Fake Image 20 - Predicted Class: 1, True Label: 1\n",
      "Saved Grad-CAM visualization for fake image as gradcam_fake_outputs/gradcam_fake_output_20.png\n"
     ]
    }
   ],
   "source": [
    "root_dir = 'WildRF'  # Replace with actual path\n",
    "test_dataset = DeepfakeDataset(root_dir=root_dir, split='test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "from torchcam.methods import GradCAM\n",
    "# Define Model\n",
    "model = DeepfakeClassifier().to(device)\n",
    "\n",
    "# Dynamically calculate the flattened size for sobel_cnn\n",
    "sobel_input_shape = (3, 299, 299)  # Assuming Sobel image size is 299x299\n",
    "model.initialize_sobel_linear(input_shape=sobel_input_shape)\n",
    "\n",
    "# Load pretrained weights\n",
    "model.load_state_dict(torch.load(\"best_model_module1.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Use TorchCAM's GradCAM\n",
    "# Replace with the correct convolutional layer from your model\n",
    "target_layer = \"xception.block5.rep.4.pointwise\"  # Example convolutional layer\n",
    "grad_cam = GradCAM(model, target_layer)\n",
    "import os\n",
    "\n",
    "def gradcam_visualization_on_fake_images(model, loader, grad_cam, num_images=5, save_dir=\"gradcam_outputs\"):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    images_processed = 0\n",
    "\n",
    "    # Create directory to save Grad-CAM outputs\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for images, sobel_images, yolo_features, landmarks, labels in loader:\n",
    "        images, sobel_images, yolo_features, landmarks, labels = (\n",
    "            images.to(device),\n",
    "            sobel_images.to(device),\n",
    "            yolo_features.to(device),\n",
    "            landmarks.to(device),\n",
    "            labels.to(device),\n",
    "        )\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            # Process only fake images (label == 1)\n",
    "            if labels[i].item() != 1:\n",
    "                continue  # Skip non-fake images\n",
    "\n",
    "            if images_processed >= num_images:\n",
    "                return  # Stop after visualizing `num_images`\n",
    "\n",
    "            input_image = images[i].unsqueeze(0)\n",
    "            sobel_image = sobel_images[i].unsqueeze(0)\n",
    "            yolo_feature = yolo_features[i].unsqueeze(0)\n",
    "            landmark = landmarks[i].unsqueeze(0)\n",
    "\n",
    "            # Forward pass to get predictions\n",
    "            outputs = model(input_image, sobel_image, yolo_feature, landmark)\n",
    "            pred_class = outputs.argmax(dim=1).item()\n",
    "\n",
    "            print(f\"Fake Image {images_processed + 1} - Predicted Class: {pred_class}, True Label: {labels[i].item()}\")\n",
    "\n",
    "            # Generate Grad-CAM heatmap\n",
    "            activation_map = grad_cam(class_idx=pred_class, scores=outputs)  # Explicitly pass class_idx and scores\n",
    "\n",
    "            # Remove batch dimension for visualization\n",
    "            heatmap = activation_map[0].squeeze().cpu().numpy()  # Shape: (19, 19)\n",
    "\n",
    "            # Resize heatmap to match input image dimensions\n",
    "            heatmap_resized = cv2.resize(heatmap, (299, 299))  # Assuming the input image size is 299x299\n",
    "\n",
    "            # Normalize heatmap for better visualization\n",
    "            heatmap_resized = (heatmap_resized - heatmap_resized.min()) / (heatmap_resized.max() - heatmap_resized.min())\n",
    "\n",
    "            input_image_vis = to_pil_image(input_image.squeeze().cpu())\n",
    "\n",
    "            # Plot and save the images\n",
    "            plt.figure(figsize=(10, 5))\n",
    "\n",
    "            # Original Image\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(input_image_vis)\n",
    "            plt.title(\"Original Image\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Heatmap\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(heatmap_resized, cmap=\"jet\")\n",
    "            plt.title(\"Grad-CAM Heatmap\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Overlayed Image\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(input_image_vis)\n",
    "            plt.imshow(heatmap_resized, cmap=\"jet\", alpha=0.5)  # Overlay heatmap\n",
    "            plt.title(\"Overlay\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Save the figure\n",
    "            output_path = os.path.join(save_dir, f\"gradcam_fake_output_{images_processed + 1}.png\")\n",
    "            plt.savefig(output_path, bbox_inches=\"tight\")\n",
    "            print(f\"Saved Grad-CAM visualization for fake image as {output_path}\")\n",
    "\n",
    "            plt.close()\n",
    "            images_processed += 1\n",
    "\n",
    "\n",
    "# Apply Grad-CAM only on fake images from the test dataset\n",
    "gradcam_visualization_on_fake_images(model, test_loader, grad_cam, num_images=20, save_dir=\"gradcam_fake_outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

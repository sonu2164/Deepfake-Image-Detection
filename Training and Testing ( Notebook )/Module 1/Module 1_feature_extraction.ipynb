{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting 3 level of features from resnet50 for hierarchical cross feature fusionm + Combining with ViT output and feature fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import PosixPath\n",
    "image_path = PosixPath(\"e4e\")  # Standard quotes\n",
    "train_dir = image_path / \"train\"  # Standard quotes\n",
    "test_dir = image_path / \"val\"  # Standard quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T03:39:33.300276Z",
     "iopub.status.busy": "2024-10-25T03:39:33.299405Z",
     "iopub.status.idle": "2024-10-25T03:39:59.185420Z",
     "shell.execute_reply": "2024-10-25T03:39:59.184646Z",
     "shell.execute_reply.started": "2024-10-25T03:39:33.300234Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 2/2 [00:00<00:00, 71.46it/s]\n",
      "Loading dataset: 100%|██████████| 2/2 [00:00<00:00, 2449.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, limit=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "        for label_folder in tqdm(['0_real', '1_fake'], desc=\"Loading dataset\"):\n",
    "            full_path = os.path.join(root_dir, label_folder)\n",
    "            for idx, file_name in enumerate(os.listdir(full_path)):\n",
    "                if limit and idx >= limit:\n",
    "                    break  # Limit the number of files loaded\n",
    "                if file_name.endswith(('.jpg', '.png', '.jpeg')):  # Ensure image files\n",
    "                    self.image_files.append(os.path.join(full_path, file_name))\n",
    "                    if 'real' in label_folder:\n",
    "                        self.labels.append(0)  # Label 0 for real images\n",
    "                    else:\n",
    "                        self.labels.append(1)  # Label 1 for fake images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Ensure image is 3 channels\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Data transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Paths to the training and validation directories\n",
    "train_dir = \"e4e/train\"\n",
    "val_dir = \"e4e/val\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CustomDataset(root_dir=train_dir, transform=data_transforms)\n",
    "val_dataset = CustomDataset(root_dir=val_dir, transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: certifi in /scratch/user/nkolloju/venv_name/lib/python3.6/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "import certifi\n",
    "\n",
    "# Set the SSL context to use certifi's certificates\n",
    "ssl._create_default_https_context = ssl.create_default_context\n",
    "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:28:18.865167Z",
     "iopub.status.busy": "2024-10-25T06:28:18.864812Z",
     "iopub.status.idle": "2024-10-25T06:40:54.399464Z",
     "shell.execute_reply": "2024-10-25T06:40:54.398508Z",
     "shell.execute_reply.started": "2024-10-25T06:28:18.865138Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/job.12028134/ipykernel_198645/43671950.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the saved model\n",
      "/tmp/job.12028134/ipykernel_198645/43671950.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_vit_weights = torch.load(pretrained_weights_path, map_location=device)\n",
      "Loading dataset: 100%|██████████| 2/2 [00:00<00:00, 564.40it/s]\n",
      "Loading dataset: 100%|██████████| 2/2 [00:00<00:00, 1927.97it/s]\n",
      "Loading dataset: 100%|██████████| 2/2 [00:00<00:00, 726.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Train Dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1400/1400 [00:46<00:00, 29.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to features_collabDiff/train_features.csv\n",
      "Processing Validation Dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 200/200 [00:06<00:00, 30.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to features_collabDiff/val_features.csv\n",
      "Processing Test Dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 400/400 [00:39<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to features_collabDiff/test_features.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data transformations (ResNet-style transformations)\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom dataset class\n",
    "# Custom dataset class\n",
    "class CustomDatasetNew(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, limit=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "        for label_folder in tqdm(['real', 'fake'], desc=\"Loading dataset\"):\n",
    "            full_path = os.path.join(root_dir, label_folder)\n",
    "            for idx, file_name in enumerate(os.listdir(full_path)):\n",
    "                if limit and idx >= limit:\n",
    "                    break  # Limit the number of files loaded\n",
    "                if file_name.endswith(('.jpg', '.png', '.jpeg')):  # Ensure image files\n",
    "                    self.image_files.append(os.path.join(full_path, file_name))\n",
    "                    if 'real' in label_folder:\n",
    "                        self.labels.append(0)  # Label 0 for real images\n",
    "                    else:\n",
    "                        self.labels.append(1)  # Label 1 for fake images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]  # This is a string path\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Ensure image is 3 channels\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_path  # Ensure that the path returned is a string (not a tensor)\n",
    "\n",
    "\n",
    "# Load ResNet model and capture features\n",
    "def load_saved_resnet_model(model_path):\n",
    "    model = torchvision.models.resnet50(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)  # Binary classification (real/fake)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))  # Load the saved model\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Hook functions to capture low, mid, and high-level features\n",
    "    model.layer1[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, low_level_features))\n",
    "    model.layer3[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, mid_level_features))\n",
    "    model.layer4[0].register_forward_hook(lambda m, i, o: hook_fn(m, i, o, high_level_features))\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Hook functions to capture ResNet features\n",
    "low_level_features, mid_level_features, high_level_features = [], [], []\n",
    "\n",
    "def hook_fn(module, input, output, storage_list):\n",
    "    storage_list.append(output.clone().detach())\n",
    "\n",
    "# Define linear layers to convert ResNet features to 768 dimensions\n",
    "# Define linear layers to convert ResNet features to 768 dimensions\n",
    "low_to_768 = nn.Linear(256, 768).to(device)   # For low-level features\n",
    "mid_to_768 = nn.Linear(1024, 768).to(device)  # For mid-level features\n",
    "high_to_768 = nn.Linear(2048, 768).to(device) # For high-level features\n",
    "\n",
    "def extract_resnet_features(model, image):\n",
    "    low_level_features.clear()\n",
    "    mid_level_features.clear()\n",
    "    high_level_features.clear()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "        model(image)\n",
    "\n",
    "    # Pool ResNet features and map to 768 dimensions\n",
    "    low_pooled = F.adaptive_avg_pool2d(low_level_features[-1].to(device), (1, 1)).squeeze()\n",
    "    mid_pooled = F.adaptive_avg_pool2d(mid_level_features[-1].to(device), (1, 1)).squeeze()\n",
    "    high_pooled = F.adaptive_avg_pool2d(high_level_features[-1].to(device), (1, 1)).squeeze()\n",
    "\n",
    "    low_768 = low_to_768(low_pooled)   # Shape [1, 768]\n",
    "    mid_768 = mid_to_768(mid_pooled)   # Shape [1, 768]\n",
    "    high_768 = high_to_768(high_pooled) # Shape [1, 768]\n",
    "\n",
    "    return low_768, mid_768, high_768\n",
    "\n",
    "\n",
    "# Function to preprocess the image using ViT's transforms\n",
    "def pipeline_preprocessor():\n",
    "    vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    return vit_weights.transforms()\n",
    "\n",
    "# Function to extract ViT embeddings\n",
    "def get_vit_embedding(vit_model, image_path):\n",
    "    preprocessing = pipeline_preprocessor()  # Preprocessing from ViT\n",
    "    img = Image.open(image_path).convert(\"RGB\")  # Ensure we load image by path (string)\n",
    "    img = preprocessing(img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feats = vit_model._process_input(img)\n",
    "        batch_class_token = vit_model.class_token.expand(img.shape[0], -1, -1)\n",
    "        feats = torch.cat([batch_class_token, feats], dim=1)\n",
    "        feats = vit_model.encoder(feats)\n",
    "        vit_hidden = feats[:, 0]  # CLS token\n",
    "    return vit_hidden\n",
    "\n",
    "# Load ViT model\n",
    "def load_vit_model(pretrained_weights_path):\n",
    "    vit_model = torchvision.models.vit_b_16(pretrained=False).to(device)\n",
    "    pretrained_vit_weights = torch.load(pretrained_weights_path, map_location=device)\n",
    "    vit_model.load_state_dict(pretrained_vit_weights, strict=False)\n",
    "    vit_model.eval()  # Set to evaluation mode\n",
    "    return vit_model\n",
    "\n",
    "# Add a sequence dimension (if missing) before applying attention\n",
    "def ensure_correct_shape(tensor):\n",
    "    if len(tensor.shape) == 2:  # If shape is [batch_size, embedding_dim]\n",
    "        tensor = tensor.unsqueeze(1)  # Add a sequence dimension: [batch_size, 1, embedding_dim]\n",
    "    elif len(tensor.shape) == 1:  # If shape is [embedding_dim]\n",
    "        tensor = tensor.unsqueeze(0).unsqueeze(1)  # Add batch and sequence dimensions: [1, 1, embedding_dim]\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# Scaled dot product attention function\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    # Ensure Q, K, and V have the correct shapes\n",
    "    Q = ensure_correct_shape(Q)  # Should be [batch_size, 1, embedding_dim]\n",
    "    K = ensure_correct_shape(K)  # Should be [batch_size, 1, embedding_dim]\n",
    "    V = ensure_correct_shape(V)  # Should be [batch_size, 1, embedding_dim]\n",
    "\n",
    "#     print(f\"Q shape after unsqueeze: {Q.shape}, K shape after unsqueeze: {K.shape}, V shape after unsqueeze: {V.shape}\")  # Debugging\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32).to(Q.device))\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    return output\n",
    "\n",
    "# Save features for each dataset (train/val/test)\n",
    "import csv\n",
    "\n",
    "# Save features for each dataset (train/val/test) as CSV\n",
    "def save_features_to_csv(model, vit_model, data_loader, save_path):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    with open(save_path, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the CSV header\n",
    "        writer.writerow([\"image_name\", \"features\", \"label\"])\n",
    "\n",
    "        for images, img_paths in tqdm(data_loader, desc=\"Extracting features\"):\n",
    "            for i in range(len(images)):\n",
    "                image = images[i].to(device)  # Move image to the correct device\n",
    "                img_path = img_paths[i]  # Image path\n",
    "\n",
    "                # Ensure img_path is a string\n",
    "                if isinstance(img_path, torch.Tensor):\n",
    "                    img_path = img_path.item() if img_path.dim() == 0 else str(img_path)\n",
    "\n",
    "                # Extract ResNet features\n",
    "                try:\n",
    "                    low_768, mid_768, high_768 = extract_resnet_features(model, image)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting ResNet features for {img_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Extract ViT features\n",
    "                try:\n",
    "                    vit_hidden = get_vit_embedding(vit_model, img_path)  # img_path should be a string\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting ViT features for {img_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Apply attention between ResNet and ViT features\n",
    "                try:\n",
    "                    output_1 = scaled_dot_product_attention(vit_hidden, low_768, low_768)\n",
    "                    output_2 = scaled_dot_product_attention(output_1, mid_768, mid_768)\n",
    "                    final_output = scaled_dot_product_attention(output_2, high_768, high_768)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error applying attention for {img_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert features to a flattened list\n",
    "                features = final_output.detach().cpu().numpy().flatten().tolist()\n",
    "\n",
    "\n",
    "                # Extract label from the image path\n",
    "                label = 0 if \"real\" in img_path else 1\n",
    "\n",
    "                # Write the row to the CSV\n",
    "                writer.writerow([os.path.basename(img_path), features, label])\n",
    "\n",
    "    print(f\"Features saved to {save_path}\")\n",
    "\n",
    "\n",
    "# Load models\n",
    "resnet_model = load_saved_resnet_model('best_model_resnet50_collabdiff.pth')\n",
    "vit_model = load_vit_model('collabdiff_vit_state_dict.pth')\n",
    "\n",
    "train_dir = \"CollabDiff/train\"\n",
    "val_dir = \"CollabDiff/val\"\n",
    "test_dir=\"CollabDiff/test\"\n",
    "\n",
    "train_dataset = CustomDatasetNew(root_dir=train_dir, transform=data_transforms)\n",
    "val_dataset = CustomDatasetNew(root_dir=val_dir, transform=data_transforms)\n",
    "test_dataset = CustomDatasetNew(root_dir=test_dir, transform=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(\"Processing Train Dataset:\")\n",
    "save_features_to_csv(resnet_model, vit_model, train_loader, save_path=\"features_collabDiff/train_features.csv\")\n",
    "\n",
    "print(\"Processing Validation Dataset:\")\n",
    "save_features_to_csv(resnet_model, vit_model, val_loader, save_path=\"features_collabDiff/val_features.csv\")\n",
    "\n",
    "print(\"Processing Test Dataset:\")\n",
    "save_features_to_csv(resnet_model, vit_model, test_loader, save_path=\"features_collabDiff/test_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:42:26.766618Z",
     "iopub.status.busy": "2024-10-25T06:42:26.765657Z",
     "iopub.status.idle": "2024-10-25T06:50:44.644144Z",
     "shell.execute_reply": "2024-10-25T06:50:44.643228Z",
     "shell.execute_reply.started": "2024-10-25T06:42:26.766556Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now handling test directory structure with subfolders (twitter, facebook, reddit)\n",
    "# test_dir = \"WildRF/test\"\n",
    "# for test_subdir in ['twitter', 'facebook', 'reddit']:\n",
    "#     test_dataset = CustomDatasetNew(root_dir=os.path.join(test_dir, test_subdir), transform=data_transforms)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "#     print(f\"Processing Test Dataset: {test_subdir}\")\n",
    "#     save_features(resnet_model, vit_model, test_loader, save_dir=f\"features/test/{test_subdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:59:36.834863Z",
     "iopub.status.busy": "2024-10-25T06:59:36.834496Z",
     "iopub.status.idle": "2024-10-25T06:59:37.869059Z",
     "shell.execute_reply": "2024-10-25T06:59:37.868364Z",
     "shell.execute_reply.started": "2024-10-25T06:59:36.834831Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def folder_to_zip(folder_path, zip_name):\n",
    "    # Check if the folder exists\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(\"The folder does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Create a zip file from the folder\n",
    "    shutil.make_archive(zip_name, 'zip', folder_path)\n",
    "    print(f\"Folder '{folder_path}' has been successfully zipped to '{zip_name}.zip'.\")\n",
    "\n",
    "# Example usage\n",
    "folder_to_zip('/kaggle/working/features', 'features')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 146776,
     "modelInstanceId": 123715,
     "sourceId": 145891,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 146847,
     "modelInstanceId": 123786,
     "sourceId": 145964,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30788,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

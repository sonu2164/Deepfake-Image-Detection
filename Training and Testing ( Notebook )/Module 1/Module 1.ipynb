{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the code for taking extracted features from the Module 1 i.e. ViT B/16 + ResNet 50 + Calibration and then using DNN over it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_req_set(path):\n",
    "    df = pd.read_csv(path)\n",
    "    features_df = df['features'].str.strip('[]').str.split(',', expand=True)\n",
    "    features_df = features_df.astype(float)\n",
    "    features_df.columns = [f'feature_{i}' for i in range(features_df.shape[1])]\n",
    "    df_expanded = pd.concat([features_df, df['label']], axis=1)\n",
    "    X = df_expanded.drop(columns=['label'])\n",
    "    y = df_expanded['label']\n",
    "    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    temp_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    return temp_loader\n",
    "\n",
    "'''\n",
    "\n",
    "# Here this part extracts the Module 1 features saved in a csv and works over it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogoJf12DbAmR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: Train Loss: 0.6937, Train Accuracy: 51.95%, Val Loss: 0.6859, Val Accuracy: 50.00%\n",
      "Epoch 2/30: Train Loss: 0.6128, Train Accuracy: 69.47%, Val Loss: 0.4327, Val Accuracy: 81.66%\n",
      "Epoch 3/30: Train Loss: 0.3712, Train Accuracy: 84.11%, Val Loss: 0.3344, Val Accuracy: 86.93%\n",
      "Epoch 4/30: Train Loss: 0.3193, Train Accuracy: 86.36%, Val Loss: 0.2890, Val Accuracy: 86.43%\n",
      "Epoch 5/30: Train Loss: 0.2760, Train Accuracy: 88.90%, Val Loss: 0.2992, Val Accuracy: 87.69%\n",
      "Epoch 6/30: Train Loss: 0.2777, Train Accuracy: 88.72%, Val Loss: 0.3000, Val Accuracy: 87.44%\n",
      "Epoch 7/30: Train Loss: 0.2638, Train Accuracy: 89.75%, Val Loss: 0.2510, Val Accuracy: 90.20%\n",
      "Epoch 8/30: Train Loss: 0.2217, Train Accuracy: 91.41%, Val Loss: 0.2758, Val Accuracy: 89.95%\n",
      "Epoch 9/30: Train Loss: 0.2045, Train Accuracy: 92.85%, Val Loss: 0.2536, Val Accuracy: 89.95%\n",
      "Epoch 10/30: Train Loss: 0.1924, Train Accuracy: 92.92%, Val Loss: 0.2656, Val Accuracy: 88.19%\n",
      "Early stopping triggered.\n",
      "Training and validation curves saved as 'training_validation_curves.png'.\n",
      "\n",
      "Testing on individual test sets:\n",
      "Facebook - Loss: 0.3717, Accuracy: 85.00%, Precision: 0.91, Recall: 0.78, F1 Score: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/job.12092825/ipykernel_175166/2364216840.py:282: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model_module2.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit - Loss: 0.2400, Accuracy: 91.20%, Precision: 0.92, Recall: 0.90, F1 Score: 0.91\n",
      "Twitter - Loss: 0.3862, Accuracy: 82.90%, Precision: 0.94, Recall: 0.79, F1 Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and prepare DataLoader\n",
    "def get_req_set(path):\n",
    "    df = pd.read_csv(path)\n",
    "    features_df = df['features'].str.strip('[]').str.split(',', expand=True)\n",
    "    features_df = features_df.astype(float)\n",
    "    features_df.columns = [f'feature_{i}' for i in range(features_df.shape[1])]\n",
    "    df_expanded = pd.concat([features_df, df['label']], axis=1)\n",
    "    X = df_expanded.drop(columns=['label'])\n",
    "    y = df_expanded['label']\n",
    "    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    temp_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    return temp_loader\n",
    "\n",
    "# Paths to datasets\n",
    "train_loader = get_req_set('/scratch/user/nkolloju/GENAI/WildRF_Features/train_features.csv')\n",
    "val_loader = get_req_set('/scratch/user/nkolloju/GENAI/WildRF_Features/val_features.csv')\n",
    "test_loaders = {\n",
    "    'Facebook': get_req_set('/scratch/user/nkolloju/GENAI/WildRF_Features/facebook_features.csv'),\n",
    "    'Reddit': get_req_set('/scratch/user/nkolloju/GENAI/WildRF_Features/reddit_features.csv'),\n",
    "    'Twitter': get_req_set('/scratch/user/nkolloju/GENAI/WildRF_Features/twitter_features.csv')\n",
    "}\n",
    "\n",
    "# Define the DNN model\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim, dropout_prob=0.2):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for batch_X, batch_y in loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == batch_y).sum().item()\n",
    "        total_samples += batch_y.size(0)\n",
    "\n",
    "        y_true.extend(batch_y.numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "    loss = running_loss / len(loader)\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    return loss, accuracy, precision, recall, f1\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(batch_y.numpy())\n",
    "            y_pred.extend(predicted.numpy())\n",
    "\n",
    "    loss = running_loss / len(loader)\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    return loss, accuracy, precision, recall, f1\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "input_dim = 768\n",
    "hidden_dim_1 = 128\n",
    "hidden_dim_2 = 256\n",
    "output_dim = 2\n",
    "model = DNN(input_dim, hidden_dim_1, hidden_dim_2, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc, train_prec, train_rec, train_f1 = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1 = validate(model, val_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}: \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model_module1.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "            \n",
    "            # Plot loss and accuracy curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_validation_curves_module1_WildRf.png\")\n",
    "print(\"Training and validation curves saved as 'training_validation_curves.png'.\")\n",
    "# Test model on different test sets\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model_module1.pth\"))\n",
    "print(\"\\nTesting on individual test sets:\")\n",
    "for name, loader in test_loaders.items():\n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1 = validate(model, loader, criterion)\n",
    "    print(f\"{name} - Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%, \"\n",
    "          f\"Precision: {test_prec:.2f}, Recall: {test_rec:.2f}, F1 Score: {test_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
